'solo_Heals',
'solo_Revives',
'solo_Boosts',
'solo_DamageDealt',
'solo_DBNOs',
'duo_KillDeathRatio',
'duo_WinRatio',
'duo_TimeSurvived',
'duo_RoundsPlayed',
'duo_Wins',
'duo_WinTop10Ratio',
'duo_Top10s',
'duo_Top10Ratio',
'duo_Losses',
'duo_Rating',
'duo_BestRating',
'duo_DamagePg',
'duo_HeadshotKillsPg',
'duo_HealsPg',
'duo_KillsPg',
'duo_MoveDistancePg',
'duo_RevivesPg',
'duo_RoadKillsPg',
'duo_TeamKillsPg',
'duo_TimeSurvivedPg',
'duo_Top10sPg',
'duo_Kills',
'duo_Assists',
'duo_Suicides',
'duo_TeamKills',
'duo_HeadshotKills',
'duo_HeadshotKillRatio',
'duo_VehicleDestroys',
'duo_RoadKills',
'duo_DailyKills',
'duo_WeeklyKills',
'duo_RoundMostKills',
'duo_MaxKillStreaks',
'duo_WeaponAcquired',
'duo_Days',
'duo_LongestTimeSurvived',
'duo_MostSurvivalTime',
'duo_AvgSurvivalTime',
'duo_WinPoints',
'duo_WalkDistance',
'duo_RideDistance',
'duo_MoveDistance',
'duo_AvgWalkDistance',
'duo_AvgRideDistance',
'duo_LongestKill',
'duo_Heals',
'duo_Revives',
'duo_Boosts',
'duo_DamageDealt',
'duo_DBNOs',
'squad_KillDeathRatio',
'squad_WinRatio',
'squad_TimeSurvived',
'squad_RoundsPlayed',
'squad_Wins',
'squad_WinTop10Ratio',
'squad_Top10s',
'squad_Top10Ratio',
'squad_Losses',
'squad_Rating',
'squad_BestRating',
'squad_DamagePg',
'squad_HeadshotKillsPg',
'squad_HealsPg',
'squad_KillsPg',
'squad_MoveDistancePg',
'squad_RevivesPg',
'squad_RoadKillsPg',
'squad_TeamKillsPg',
'squad_TimeSurvivedPg',
'squad_Top10sPg',
'squad_Kills',
'squad_Assists',
'squad_Suicides',
'squad_TeamKills',
'squad_HeadshotKills',
'squad_HeadshotKillRatio',
'squad_VehicleDestroys',
'squad_RoadKills',
'squad_DailyKills',
'squad_WeeklyKills',
'squad_RoundMostKills',
'squad_MaxKillStreaks',
'squad_WeaponAcquired',
'squad_Days',
'squad_LongestTimeSurvived',
'squad_MostSurvivalTime',
'squad_AvgSurvivalTime',
'squad_WinPoints',
'squad_WalkDistance',
'squad_RideDistance',
'squad_MoveDistance',
'squad_AvgWalkDistance',
'squad_AvgRideDistance',
'squad_LongestKill',
'squad_Heals',
'squad_Revives',
'squad_Boosts',
'squad_DamageDealt',
'squad_DBNOs')
names(wk8_dat) = wk8_names
dat = wk8_dat %>% dplyr::select(-V1)
View(wk8_dat)
dat = wk8_dat %>% dplyr::select(starts_with('solo'))
summary(dat)
dat = wk8_dat %>% dplyr::select(starts_with('solo')) %>% scale()
summary(dat)
kTest = NbClust(dat, method = "kmeans")
dat = wk8_dat %>% dplyr::select(starts_with('solo')) # %>% scale()
summary(dat)
dat$solo_WeaponAcquired
summary(dat$solo_WeaponAcquired)
unique(dat$solo_WeaponAcquired)
table(dat$solo_WeaponAcquired)
table(dat$solo_WinRatio)
table(dat$solo_RoadKillsPg)
table(dat$solo_TeamKills)
table(dat$solo_DBNOs)
table(dat$solo_Revives)
summary(wk8_dat)
wk8_dat %>% dplyr::select(starts_with('solo')) %>% summary
table(dat$solo_Revives)
table(dat$solo_WeaponAcquired)
wk8_names = c('player_name', 'tracker_id', 'solo_KillDeathRatio',
'solo_WinRatio', 'solo_TimeSurvived', 'solo_RoundsPlayed',
'solo_Wins', 'solo_WinTop10Ratio', 'solo_Top10s',
'solo_Top10Ratio', 'solo_Losses', 'solo_Rating',
'solo_BestRating', 'solo_DamagePg', 'solo_HeadshotKillsPg',
'solo_HealsPg', 'solo_KillsPg', 'solo_MoveDistancePg',
'solo_RevivesPg', 'solo_RoadKillsPg', 'solo_TeamKillsPg',
'solo_TimeSurvivedPg', 'solo_Top10sPg', 'solo_Kills',
'solo_Assists', 'solo_Suicides', 'solo_TeamKills',
'solo_HeadshotKills', 'solo_HeadshotKillRatio', 'solo_VehicleDestroys',
'solo_RoadKills', 'solo_DailyKills', 'solo_WeeklyKills',
'solo_RoundMostKills', 'solo_MaxKillStreaks', 'solo_WeaponAcquired',
'solo_Days', 'solo_LongestTimeSurvived', 'solo_MostSurvivalTime',
'solo_AvgSurvivalTime', 'solo_WinPoints', 'solo_WalkDistance',
'solo_RideDistance', 'solo_MoveDistance', 'solo_AvgWalkDistance',
'solo_AvgRideDistance', 'solo_LongestKill', 'solo_Heals',
'solo_Revives', 'solo_Boosts', 'solo_DamageDealt',
'solo_DBNOs', 'duo_KillDeathRatio', 'duo_WinRatio',
'duo_TimeSurvived', 'duo_RoundsPlayed', 'duo_Wins',
'duo_WinTop10Ratio', 'duo_Top10s', 'duo_Top10Ratio',
'duo_Losses', 'duo_Rating', 'duo_BestRating',
'duo_DamagePg', 'duo_HeadshotKillsPg', 'duo_HealsPg',
'duo_KillsPg', 'duo_MoveDistancePg', 'duo_RevivesPg',
'duo_RoadKillsPg', 'duo_TeamKillsPg', 'duo_TimeSurvivedPg',
'duo_Top10sPg', 'duo_Kills', 'duo_Assists',
'duo_Suicides', 'duo_TeamKills', 'duo_HeadshotKills',
'duo_HeadshotKillRatio', 'duo_VehicleDestroys', 'duo_RoadKills',
'duo_DailyKills', 'duo_WeeklyKills', 'duo_RoundMostKills',
'duo_MaxKillStreaks', 'duo_WeaponAcquired', 'duo_Days',
'duo_LongestTimeSurvived', 'duo_MostSurvivalTime', 'duo_AvgSurvivalTime',
'duo_WinPoints', 'duo_WalkDistance', 'duo_RideDistance',
'duo_MoveDistance', 'duo_AvgWalkDistance', 'duo_AvgRideDistance',
'duo_LongestKill', 'duo_Heals', 'duo_Revives',
'duo_Boosts', 'duo_DamageDealt', 'duo_DBNOs',
'squad_KillDeathRatio', 'squad_WinRatio', 'squad_TimeSurvived',
'squad_RoundsPlayed', 'squad_Wins', 'squad_WinTop10Ratio',
'squad_Top10s', 'squad_Top10Ratio', 'squad_Losses',
'squad_Rating', 'squad_BestRating', 'squad_DamagePg',
'squad_HeadshotKillsPg', 'squad_HealsPg', 'squad_KillsPg',
'squad_MoveDistancePg', 'squad_RevivesPg', 'squad_RoadKillsPg',
'squad_TeamKillsPg', 'squad_TimeSurvivedPg', 'squad_Top10sPg',
'squad_Kills', 'squad_Assists', 'squad_Suicides',
'squad_TeamKills', 'squad_HeadshotKills', 'squad_HeadshotKillRatio',
'squad_VehicleDestroys', 'squad_RoadKills', 'squad_DailyKills',
'squad_WeeklyKills', 'squad_RoundMostKills', 'squad_MaxKillStreaks',
'squad_WeaponAcquired', 'squad_Days', 'squad_LongestTimeSurvived',
'squad_MostSurvivalTime', 'squad_AvgSurvivalTime', 'squad_WinPoints',
'squad_WalkDistance', 'squad_RideDistance', 'squad_MoveDistance',
'squad_AvgWalkDistance', 'squad_AvgRideDistance', 'squad_LongestKill',
'squad_Heals', 'squad_Revives', 'squad_Boosts',
'squad_DamageDealt', 'squad_DBNOs')
names(wk8_dat) = wk8_names
dat = wk8_dat %>% dplyr::select(starts_with('solo'))  %>% scale()
summary(dat)
#Load libraries
library(tidyverse)
library(plm)
library(tidyr)
library(ggplot2)
library(scales)
library(readstata13)
library(corrplot)
library(reshape)
library(dplyr)
library(corrplot)
library(gridExtra)
library(rpart)
library(partykit)
library(caret)
library(mlbench)
library(tidyverse)
library(rminer)
library(randomForest)
library(reshape2)
library(MASS)
library(e1071)
library(wsrf)
library(kernlab)
# set working directory and import data
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
spamdata <- read.csv("spambase.data", header=FALSE)
dim(spamdata)
glimpse(spamdata)
# Find NA per Column
if (sum(is.na(spamdata)) == 0) {
print(paste("The count of NA in dataset =", sum(is.na(spamdata))))
} else {
cnt <- ncol(spamdata)
for (i in 1:cnt) {
print(colnames(spamdata)[i])
print(sum(is.na(spamdata[,i])))
}
}
#create a list of variable names
newnames <- c("freq_make", "freq_address", "freq_all", "freq_3d", "freq_our", "freq_over", "freq_remove", "freq_internet",
"freq_order", "freq_mail", "freq_receive", "freq_will", "freq_people", "freq_report", "freq_addresses", "freq_free",
"freq_business", "freq_email", "freq_you", "freq_credit", "freq_your", "freq_font", "freq_000", "freq_money",
"freq_hp", "freq_hpl", "freq_george", "freq_650", "freq_lab", "freq_labs", "freq_telnet", "freq_857",
"freq_data", "freq_415", "freq_85", "freq_technology", "freq_1999", "freq_parts", "freq_pm", "freq_direct",
"freq_vcs", "freq_meeting", "freq_original", "freq_project", "freq_re", "freq_edu", "freq_table", "freq_conference",
"freq_semicolon", "freq_parentheses", "freq_bracket", "freq_exclamation_point", "freq_dollar_sign", "freq_pound_sign",
"caps_len_average", "caps_len_longest", "caps_len_total", "spam")
# set the variable names
names(spamdata) <- newnames
# tidy the data
spamdata$caps_len_longest <- as.double(spamdata$caps_len_longest)
spamdata$caps_len_total <- as.double(spamdata$caps_len_total)
spamdata <- spamdata %>% dplyr::select(spam,everything())
spamdata$spam <- as.factor(ifelse(spamdata$spam==1,"Y","N"))
glimpse(spamdata)
names(spamdata)
table(spamdata$spam)
#manually look for outliers
# standard deviation methods
# 2 SD beyond the mean covers 95% of a normally distributed variable
# 3 SD +/- the mean covers 99.7% of a normally distributed variable
for (i in 2:ncol(spamdata)){
temp <- spamdata[,i] > (mean(spamdata[,i] + 3*sd(spamdata[,i])))
temp2 <- round(table(temp)[2]/nrow(spamdata) * 100,2)
temp2
print(paste(temp2, "pecent of the", names(spamdata)[i], "data exceeds 3 SD from the mean"))
}
#sapply(dat_num, function(d) (d > (mean(d) + 2*sd(d))))
#sapply(dat_num, function(d) which(d > (mean(d) + 2*sd(d))))
#Rescale the numeric data
spamdata2 <- spamdata
rescale_x <- function(x){(x-min(x))/(max(x)-min(x))}
for (i in 2:ncol(spamdata)){
spamdata2[[i]] <- rescale_x(spamdata[[i]])
}
summary(spamdata2)
######################################################
#UNCOMMENT THIS CODE TO USED SCALED DATA IN OUR MODELS
######################################################
spamdata <- spamdata2
SEED <- 0 # random seed
REPS <- 50 # training replications
RATIO <- 0.6 # train-test split ratio
NTREE <- 100 # number of trees
# set random seed to use same data splits
set.seed(SEED)
# split data for train and test
id = holdout(spamdata$spam, ratio=split_ratio, mode='stratified')
split_ratio = 0.6
# split data for train and test
id = holdout(spamdata$spam, ratio=split_ratio, mode='stratified')
View(id)
id[["tr"]]
set.seed(SEED)
id = holdout(spamdata$spam, ratio=split_ratio, mode='stratified')
id[["tr"]]
SEED <- 0 # random seed
REPS <- 50 # training replications
RATIO <- 0.6 # train-test split ratio
NTREE <- 100 # number of trees
###########################################################
# Generic wrapper function for training and prediction
#
# @param name
#     name of the model e.g. "Decision Tree"
# @param build_model
#     function to perform the model-specific training
#     e.g. build_model <- function(data) {
#              return(rpart(Y ~ ., data = data))
#          }
#
# @param split_ratio
#     train to test split ratio e.g. 0.75
#
# @param predict_call
#     variable to control how predict is called
#
# @returns
#     list containing the error % for each rep and the
#     model for each rep
#
###########################################################
train_wrapper <- function(name,
build_model,
split_ratio = 0.6,
predict_call = 1) {
# set random seed to use same data splits
set.seed(SEED)
# vector to store error for each repetition
errors <- rep(0, REPS)
# vector to store models for each repetition
models <- vector(mode = "list", length = REPS)
# vector to store yhat predictions for each repetition
yhats <- vector(mode = "list", length = REPS)
# vector to store test sets
tests <- vector(mode = "list", length = REPS)
for (r in 1:length(errors)) {
# split data for train and test
id = holdout(spamdata$spam, ratio=split_ratio, mode='stratified')
train <- spamdata[id$tr,]
test <- spamdata[id$ts,]
tests[[r]] <- test
# build model on training set
model <- build_model(train)
# store model
models[[r]] <- model
# predict on test set
yhat <- switch (predict_call,
predict(model, test %>% dplyr::select(-spam), type = 'class'),
predict(model, test %>% dplyr::select(-spam)),
predict(model, test %>% dplyr::select(-spam))$class)
# store predictions
yhats[[r]] <- yhat
# store error
errors[r] <- mean(yhat != test$spam)
# print progress
cat(name, "[rep]:", r, "[error]:", errors[r], "\n")
}
# print confusion matrix for most accurate model
index <- which.min(errors)
print(confusionMatrix(table(yhats[[index]], tests[[index]]$spam)))
return(list("errors" = errors, "models" = models))
}
build_regular_tree <- function(training_data) {
rpart(spam ~ ., data = training_data)
}
regular_tree_errors <- train_wrapper("Regular Trees", build_regular_tree)
build_bagged_tree <- function(training_data) {
model <- randomForest(spam ~ .,
data = training_data,
mtry = ncol(training_data) - 1,
ntree = NTREE)
return(model)
}
bagged_tree_errors <- train_wrapper("Bagged Trees", build_bagged_tree)
#store regular tree error data as RData files
save(regular_tree_errors, file='regular_tree_errors.rdata')
#load regular tree error data
load(file='regular_tree_errors.rdata')
#store baggede tree error data as RData files
save(bagged_tree_errors, file='bagged_tree_errors.rdata')
#load regular tree error data
load(file='bagged_tree_errors.rdata')
build_rf <- function(training_data) {
model <- randomForest(spam ~ .,
data = training_data,
mtry = sqrt(ncol(training_data) - 1),
ntree = NTREE)
return(model)
}
rf_errors <- train_wrapper("Random Forest", build_rf)
#store random forest error data as RData files
save(rf_errors, file='rf_errors.rdata')
#load random forest error data
load(file='rf_errors.rdata')
build_weighted_rf <- function(training_data) {
model <- randomForest(spam ~ .,
data = training_data,
ntree = NTREE,
classwt = c(.8, .9))
return(model)
}
weighted_rf_errors <- train_wrapper("Weighted Random Forest", build_weighted_rf)
#store weighted random forest error data as RData files
save(weighted_rf_errors, file='weighted_rf_errors.rdata')
#load weighted random forest error data
load(file='weighted_rf_errors.rdata')
build_balanced_rf <- function(training_data) {
minority_class_size <- min(table(training_data$spam))
model <- randomForest(spam ~ .,
data = training_data,
ntree = NTREE,
sampsize = minority_class_size)
return(model)
}
balanced_rf_errors <- train_wrapper("Balanced Random Forest", build_balanced_rf)
#store balanced random forest error data as RData files
save(balanced_rf_errors, file='balanced_rf_errors.rdata')
#load balanced random forest error data
load(file='balanced_rf_errors.rdata')
build_lda <- function(training_data) {
model <- lda(spam ~ ., data = training_data)
return(model)
}
lda_errors <- train_wrapper("LDA",
build_lda,
split_ratio = RATIO,
predict_call = 3)$errors
#store LDA error data as RData files
save(lda_errors, file='lda_errors.rdata')
#load LDA error data
load(file='lda_errors.rdata')
build_csvm <- function(training_data) {
model <- svm(spam ~ ., data = training_data, type = "C-classification")
return(model)
}
csvm_errors <- train_wrapper("C-SVM",
build_csvm,
split_ratio = RATIO)$errors
#store LDA error data as RData files
save(lda_errors, file='lda_errors.rdata')
#load LDA error data
load(file='lda_errors.rdata')
#store C-SVM error data as RData files
save(csvm_errors, file='csvm_errors.rdata')
#load C-SVM error data
load(file='csvm_errors.rdata')
build_nusvm <- function(training_data) {
model <- svm(spam ~ ., data = training_data, type = "nu-classification")
return(model)
}
nusvm_errors <- train_wrapper("nu-SVM",
build_nusvm,
split_ratio = RATIO)$errors
#store C-SVM error data as RData files
save(nusvm_errors, file='nusvm_errors.rdata')
#load C-SVM error data
load(file='nusvm_errors.rdata')
# aggregate to data frame
combined_errors <- as.tibble(data.frame(
Regular = regular_tree_errors$errors,
Bagged = bagged_tree_errors$errors,
RF = rf_errors$errors,
Weighted_RF = weighted_rf_errors$errors,
Balanced_RF = balanced_rf_errors$errors,
LDA = lda_errors,
CSVM = csvm_errors,
NUSVM = nusvm_errors))
#store combined error data as RData files
save(combined_errors, file='combined_errors.rdata')
#load combined error data
load(file='combined_errors.rdata')
# plot errors
combined_errors %>%
melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
"NUSVM")) %>%
ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
xlab("Classifier") + ylab("Error %")
#load combined error data
load(file='combined_errors_UNSCALED.rdata')
# plot errors
combined_errors %>%
melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
"NUSVM")) %>%
ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
xlab("Classifier") + ylab("Error %")
#load combined error data
load(file='combined_errors_SCALED.rdata')
# plot errors
combined_errors %>%
melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
"NUSVM")) %>%
ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
xlab("Classifier") + ylab("Error %")
#load combined error data
load(file='combined_errors_SCALED.rdata')
# plot errors
combined_errors %>%
melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
"NUSVM")) %>%
ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
xlab("Classifier") + ylab("Error %")
# plot errors
combined_errors %>%
melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
"NUSVM")) %>%
ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
xlab("Classifier") + ylab("Error %")
# aggregate to data frame
combined_errors <- as.tibble(data.frame(
Regular = regular_tree_errors$errors,
Bagged = bagged_tree_errors$errors,
RF = rf_errors$errors,
Weighted_RF = weighted_rf_errors$errors,
Balanced_RF = balanced_rf_errors$errors,
LDA = lda_errors,
CSVM = csvm_errors,
NUSVM = nusvm_errors))
# plot errors
combined_errors %>%
melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
"NUSVM")) %>%
ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
xlab("Classifier") + ylab("Error %")
# set control
control <- trainControl(method = 'cv', number = 5)
# retrain the model
RF_tune_model <- train(spam ~ ., data = spamdata, method = 'rf', trControl = control,
tuneLength = 5)
