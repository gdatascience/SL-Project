---
title: 'Statistical Learning: Final Project'
author: 
- 'Mike Baietto, Dan Malee, Gerard Martinez, and Tony Galvan'
- 'West'
date: "10/3/2018"
output: pdf_document
---

#Final Project
##Description
The goal of this project is to identify and explore interesting aspects of data in a
real-world context, provide extensive explanations about each step, and report on your
analysis of the results. The final project will involve communicating results of your
analysis and technical results in a group presentation that effectively translates your
work. You will work in groups of 3-4 students for the group presentation component and
turn in an individual write-up component.

The project will consist of the following tasks:
??? Analyzing data and selecting the model best suited to solve the problem.
??? Implementing and optimizing algorithms discussed in this course.
??? Using R or Python to perform computations and analyze the results.
??? Completing a slide deck in Google Slides for the group presentation.
??? Effectively communicating methodology and results during the group presentation.

This write-up will consist of the following sections:
1.  Exploratory Data Analysis (EDA)
2.  Preprocessing
3.  Initial Models
4.  Model Optimization
5.  Figures & Conclusion

\pagebreak

##Read In spambase.data file

```{r message=FALSE}
#Start Fresh
rm(list=ls())
```

```{r results='hide', message=FALSE, warning=FALSE}
#Load libraries
library(tidyverse)
library(plm)
library(tidyr)
library(ggplot2)
library(scales)
library(readstata13)
library(corrplot)
library(reshape)
library(dplyr)
library(corrplot)
library(gridExtra)
library(rpart)
library(partykit)
library(caret)
library(mlbench)
library(tidyverse)
library(rminer)
library(randomForest)
library(reshape2)
library(MASS)
library(e1071)
library(wsrf)
library(kernlab)
```

##################################################################
#
#
# EXPLORATORY DATA ANALYSIS (EDA) SECTION
#
#
##################################################################

##LOAD THE DATA
```{r}
# set working directory and import data
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
spamdata <- read.csv("spambase.data", header=FALSE)
dim(spamdata)
```

The "spambase" data set consists of 4601 observations with 58 variables.  Let's take a closer look at the data.
```{r}
glimpse(spamdata)
```

Let's see if we have any missing data.
```{r}
# Find NA per Column
if (sum(is.na(spamdata)) == 0) {
  print(paste("The count of NA in dataset =", sum(is.na(spamdata))))
} else {
  cnt <- ncol(spamdata)
  for (i in 1:cnt) {
    print(colnames(spamdata)[i])
    print(sum(is.na(spamdata[,i])))
  }
}
```

It is nice that we do not have to deal with missing data.  

Next, we have to add names for our variables, correct a few variable data types, and move the response variable, "spam" to the first column.
```{r}
#create a list of variable names
newnames <- c("freq_make", "freq_address", "freq_all", "freq_3d", "freq_our", "freq_over", "freq_remove", "freq_internet", 
              "freq_order", "freq_mail", "freq_receive", "freq_will", "freq_people", "freq_report", "freq_addresses", "freq_free",
              "freq_business", "freq_email", "freq_you", "freq_credit", "freq_your", "freq_font", "freq_000", "freq_money", 
              "freq_hp", "freq_hpl", "freq_george", "freq_650", "freq_lab", "freq_labs", "freq_telnet", "freq_857",
              "freq_data", "freq_415", "freq_85", "freq_technology", "freq_1999", "freq_parts", "freq_pm", "freq_direct",
              "freq_vcs", "freq_meeting", "freq_original", "freq_project", "freq_re", "freq_edu", "freq_table", "freq_conference",
              "freq_semicolon", "freq_parentheses", "freq_bracket", "freq_exclamation_point", "freq_dollar_sign", "freq_pound_sign", 
              "caps_len_average", "caps_len_longest", "caps_len_total", "spam")

# set the variable names
names(spamdata) <- newnames

# tidy the data
spamdata$caps_len_longest <- as.double(spamdata$caps_len_longest)
spamdata$caps_len_total <- as.double(spamdata$caps_len_total)
spamdata <- spamdata %>% dplyr::select(spam,everything())
spamdata$spam <- as.factor(ifelse(spamdata$spam==1,"Y","N"))
```

Let's make sure our variables look better now.
```{r}
glimpse(spamdata)
```

Let's get a clean, condensed look at the variable names.
```{r}
names(spamdata)
```

Now, we can look at a table of values for the response variable.
```{r}
table(spamdata$spam)
```

We have 1813 observations that are classified as "spam" and 2788 observations that are classified as not "spam".

Next, let's check the data for outliers.
```{r}
#manually look for outliers 
# standard deviation methods 
# 2 SD beyond the mean covers 95% of a normally distributed variable 
# 3 SD +/- the mean covers 99.7% of a normally distributed variable

for (i in 2:ncol(spamdata)){
  temp <- spamdata[,i] > (mean(spamdata[,i] + 3*sd(spamdata[,i])))
  temp2 <- round(table(temp)[2]/nrow(spamdata) * 100,2)
  temp2
  print(paste(temp2, "pecent of the", names(spamdata)[i], "data exceeds 3 SD from the mean")) 
}

#sapply(dat_num, function(d) (d > (mean(d) + 2*sd(d))))
#sapply(dat_num, function(d) which(d > (mean(d) + 2*sd(d))))
```

There are some outliers in all of the variable, but nothing seems too alarming.

##HISTOGRAMS
Let's create histograms for the unscaled predictor variables
```{r}
# Create Histograms
for (i in 2:ncol(spamdata)){
  assign(paste0("h", i), ggplot(data= spamdata, aes_string(colnames(spamdata)[i], fill=spamdata$spam)) + geom_histogram() + #ggtitle(names(spamdata[i])) +
  theme_bw()+ theme(plot.title = element_text(hjust = 0.5)))
}
```

```{r fig.height=90, fig.width=10, message=FALSE}
grid.arrange(h2, h3, h4, h5, h6, h7, h8, h9, h10, h11,
             h12, h13, h14, h15, h16, h17, h18, h19, h20, h21,
             h22, h23, h24, h25, h26, h27, h28, h29, h30, h31,
             h32, h33, h34, h35, h36, h37, h38, h39, h40, h41,
             h42, h43, h44, h45, h46, h47, h48, h49, h50, h51,
             h52, h53, h54, h55, h56, h57, h58,
             ncol=2,
             top=textGrob(expression(bold(underline("Histograms"))),
                          gp=gpar(fontsize=20,font=3)))
```

Based on these histograms, it seems clear that some variables are more important than others.

##BOX PLOTS
Create box plots for the predictor variables.
```{r}
# Create Boxplots
for (i in 2:ncol(spamdata)){
  assign(paste0("p", i), 
         ggplot( data = spamdata,
                 aes_string(x = names(spamdata[1]),
                     y = names(spamdata[i]),
                               fill = spamdata$spam)) +
                 geom_boxplot() + coord_flip() +
                   theme_bw()+
                   theme(plot.title = element_text(hjust = 1.5)))
}
```


```{r fig.height=90, fig.width=10, message=FALSE}
grid.arrange(p2, p3, p4, p5, p6, p7, p8, p9, p10, p11,
             p12, p13, p14, p15, p16, p17, p18, p19, p20, p21,
             p22, p23, p24, p25, p26, p27, p28, p29, p30, p31,
             p32, p33, p34, p35, p36, p37, p38, p39, p40, p41,
             p42, p43, p44, p45, p46, p47, p48, p49, p50, p51,
             p52, p53, p54, p55, p56, p57, p58,
             ncol=2,
             top=textGrob(expression(bold(underline("Boxplots"))),
                          gp=gpar(fontsize=20,font=3)))
```

##################################################################
#
#
# PREPROCESSING SECTION
#
#
##################################################################

Let's rescale the data so that all of the values are between 0 and 1.
```{r}
#Rescale the numeric data
spamdata2 <- spamdata
rescale_x <- function(x){(x-min(x))/(max(x)-min(x))}

for (i in 2:ncol(spamdata)){
spamdata2[[i]] <- rescale_x(spamdata[[i]])
}
```

We will do a summary of the scaled data to make sure the scaling function worked as expected.
```{r}
summary(spamdata2)
```

The scaling function worked.  Let's replace spamdata with the scaled spamdata2
```{r}
spamdata_unscaled <- spamdata
spamdata <- spamdata2
```

##################################################################
#
#
# INITIAL MODELS SECTION
#
#
##################################################################

Define our constants
```{r}
SEED <- 0 # random seed
REPS <- 50 # training replications
RATIO <- 0.6 # train-test split ratio
NTREE <- 100 # number of trees
```

Define function for the common training tasks.
```{r}
###########################################################
# Generic wrapper function for training and prediction
#
# @param name
#     name of the model e.g. "Decision Tree"
# @param build_model
#     function to perform the model-specific training
#     e.g. build_model <- function(data) {
#              return(rpart(Y ~ ., data = data))
#          }
#
# @param split_ratio
#     train to test split ratio e.g. 0.75
#
# @param predict_call
#     variable to control how predict is called
#
# @returns
#     list containing the error % for each rep and the
#     model for each rep
#
###########################################################

train_wrapper <- function(name,
                          build_model,
                          split_ratio = 0.6,
                          predict_call = 1) {
  # set random seed to use same data splits
  set.seed(SEED)
  
  # vector to store error for each repetition
  errors <- rep(0, REPS)
  
  # vector to store models for each repetition
  models <- vector(mode = "list", length = REPS)
  
  # vector to store yhat predictions for each repetition
  yhats <- vector(mode = "list", length = REPS)
  
  # vector to store test sets
  tests <- vector(mode = "list", length = REPS)

  for (r in 1:length(errors)) {
    # split data for train and test
    id = holdout(spamdata$spam, ratio=split_ratio, mode='stratified')
    train <- spamdata[id$tr,]
    test <- spamdata[id$ts,]
    tests[[r]] <- test
    
    # build model on training set
    model <- build_model(train)
    
    # store model
    models[[r]] <- model
    
    # predict on test set
    yhat <- switch (predict_call,
                    predict(model, test %>% dplyr::select(-spam), type = 'class'),
                    predict(model, test %>% dplyr::select(-spam)),
                    predict(model, test %>% dplyr::select(-spam))$class)
    
    # store predictions
    yhats[[r]] <- yhat

    # store error
    errors[r] <- mean(yhat != test$spam)
    
    # print progress
    cat(name, "[rep]:", r, "[error]:", errors[r], "\n")
  }
  
  # print confusion matrix for most accurate model
  index <- which.min(errors)
  print(confusionMatrix(table(yhats[[index]], tests[[index]]$spam)))
  
  return(list("errors" = errors, "models" = models))
}
```

### Regular Trees
```{r, eval=F}
build_regular_tree <- function(training_data) {
  rpart(spam ~ ., data = training_data)
}

regular_tree_errors <- train_wrapper("Regular Trees", build_regular_tree)
```

```{r, eval = F}
#store regular tree error data as RData files
save(regular_tree_errors, file='regular_tree_errors.rdata')
```

### Bagged Trees
Use all variables at each split (mtry = p where p = number of predictors).
```{r, eval=F}
build_bagged_tree <- function(training_data) {
  model <- randomForest(spam ~ .,
                        data = training_data,
                        mtry = ncol(training_data) - 1,
                        ntree = NTREE)
  return(model)
}

bagged_tree_errors <- train_wrapper("Bagged Trees", build_bagged_tree)
```

```{r, eval = F}
#store bagged tree error data as RData files
save(bagged_tree_errors, file='bagged_tree_errors.rdata')
```

### Random Forest
Use random subset of variables at each split (mtry = square root of p).
```{r, eval=F}
build_rf <- function(training_data) {
  model <- randomForest(spam ~ .,
                        data = training_data,
                        mtry = sqrt(ncol(training_data) - 1),
                        ntree = NTREE)
  return(model)
}

rf_errors <- train_wrapper("Random Forest", build_rf)
```

```{r, eval = F}
#store random forest error data as RData files
save(rf_errors, file='rf_errors.rdata')
```

### Weighted Random Forest
Note: the selected weights were chosen through trial and error, thus may be overfitted to this particular random seed.
```{r, eval=F}
build_weighted_rf <- function(training_data) {
  model <- randomForest(spam ~ .,
                        data = training_data,
                        ntree = NTREE,
                        classwt = c(0.3, 0.8))
  return(model)
}

weighted_rf_errors <- train_wrapper("Weighted Random Forest", build_weighted_rf)
```

```{r, eval = F}
#store weighted random forest error data as RData files
save(weighted_rf_errors, file='weighted_rf_errors.rdata')
```

### Balanced Random Forest
Use under/down sampling based on size of minority class.
```{r, eval=F}
# minority class is 'Y'
build_balanced_rf <- function(training_data) {
  minority_class_size <- sum(spamdata$spam=='Y') # min(table(training_data$spam))
  
  model <- randomForest(spam ~ .,
                        data = training_data,
                        ntree = NTREE,
                        sampsize = minority_class_size)
  return(model)
}

balanced_rf_errors <- train_wrapper("Balanced Random Forest", build_balanced_rf)
```

```{r, eval = F}
#store balanced random forest error data as RData files
save(balanced_rf_errors, file='balanced_rf_errors.rdata')
```

### LDA
```{r, eval=F}
build_lda <- function(training_data) {
  model <- lda(spam ~ ., data = training_data)
  return(model)
}

lda_errors <- train_wrapper("LDA",
                            build_lda,
                            split_ratio = RATIO,
                            predict_call = 3)$errors
```

```{r, eval = F}
#store LDA error data as RData files
save(lda_errors, file='lda_errors.rdata')
```

### C-SVM
```{r, eval=F}
build_csvm <- function(training_data) {
  model <- svm(spam ~ ., data = training_data, type = "C-classification")
  return(model)
}

csvm_errors <- train_wrapper("C-SVM",
                             build_csvm,
                             split_ratio = RATIO)$errors
```

```{r, eval = F}
#store C-SVM error data as RData files
save(csvm_errors, file='csvm_errors.rdata')
```

### nu-SVM
```{r, eval=F}
build_nusvm <- function(training_data) {
  model <- svm(spam ~ ., data = training_data, type = "nu-classification")
  return(model)
}

nusvm_errors <- train_wrapper("nu-SVM",
                              build_nusvm,
                              split_ratio = RATIO)$errors
```

```{r, eval = F}
#store C-SVM error data as RData files
save(nusvm_errors, file='nusvm_errors.rdata')
```


```{r}
#load all of the error data
load(file='regular_tree_errors.rdata')
load(file='bagged_tree_errors.rdata')
load(file='rf_errors.rdata')
load(file='weighted_rf_errors.rdata')
load(file='balanced_rf_errors.rdata')
load(file='lda_errors.rdata')
load(file='csvm_errors.rdata')
load(file='nusvm_errors.rdata')
```

## Compare Results
Combine error results into comparative boxplot.
```{r, eval=F}
# aggregate to data frame
combined_errors <- as.tibble(data.frame(
  Regular = regular_tree_errors$errors,
  Bagged = bagged_tree_errors$errors,
  RF = rf_errors$errors,
  Weighted_RF = weighted_rf_errors$errors,
  Balanced_RF = balanced_rf_errors$errors,
  LDA = lda_errors,
  CSVM = csvm_errors,
  NUSVM = nusvm_errors))
```

```{r}
# plot errors
combined_errors %>%
  melt(measure.vars = c("Regular", "Bagged", "RF", "Weighted_RF", "Balanced_RF", "LDA", "CSVM",
                        "NUSVM")) %>%
  ggplot(aes(x = variable, y = value)) + geom_boxplot() + ggtitle("Classification Errors") +
  xlab("Classifier") + ylab("Error %")
```


As we can see from the above boxplots, we have Weighted Random Forest as our best performing model, with a very small advantage over a regular Random Forest.  I believe there may be some room for optimization in our models, so we will attempt optimization for both the Weighted and regular Random Forest.  We will also attempt optimization for the C-SVM model just so we can see if optimization impacts support vector machine model accuracy significantly.


##################################################################
#
#
# MODEL OPTIMIZATION SECTION
#
#
##################################################################

##RANDOMFOREST OPMTIMIZATION
Try to optimize the RandomForest model using tuneLength from the caret package.  As FYI, this tuning model will take about 5 - 10 minutes to run.  
```{r, eval=F}
# set control
control <- trainControl(method = 'cv', number = 5)
# retrain the model
RF_tune_model <- train(spam ~ ., data = spamdata, method = 'rf', trControl = control,
                       tuneLength = 5)
```


```{r, eval = F}
#store the tuned random forest model data
save(RF_tune_model, file='RF_tune_model.rdata')
```


```{r}
#load the tuned random forest model data
load(file='RF_tune_model.rdata')
```


```{r}
# summarize the model 
print(RF_tune_model)
```


```{r}
# calculate accuracy of the 'regular' random forest run earlier
mean(rf_errors$errors)
1 - mean(rf_errors$errors)
```

Our regular Random Forest ran with mtry = sqrt(ncol(training_data -1)) = sqrt(57) = 7.55, and that resulted in an accuracy of approximately 0.9494.  With our parameter tuning, we see that mtry = 15 is the best choice with a resulting accuracy of 0.9528.

##WEIGHTED RANDOMFOREST OPMTIMIZATION
Try to optimize the Weighted RandomForest model using tuneLength from the caret package, using the method 'wsrf' for Weighted Subspace Random Forest.  As FYI, the weighted RF tuning model will take about 20 - 40 minutes to run.  
```{r, eval=F}
# set control
control <- trainControl(method = 'cv', number = 5)
# retrain the model
Weight_RF_tune_model <- train(spam ~ ., data = spamdata, method = 'wsrf', trControl = control,
                       tuneLength = 5)
```


```{r, eval = F}
#store the tuned random forest model data
save(Weight_RF_tune_model, file='Weight_RF_tune_model.rdata')
```


```{r}
#load the tuned random forest model data
load(file='Weight_RF_tune_model.rdata')
```


```{r}
# summarize the model 
print(Weight_RF_tune_model)
```

Our results do worse than the orignal weighted random forest, which tells me that maybe the better way to tune a weighted random forest is to experiment with the weights.  Let's give that a try:
```{r}
#  Create a sequence to try some values
v.w1 = c(0.01, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1)
v.w2 = c(0.01, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1)
cv.for.w = matrix(0, ncol=length(v.w1), nrow=length(v.w2))
colnames(cv.for.w) = v.w1
rownames(cv.for.w) = v.w2

# set random seed to use same data splits
set.seed(SEED)
id = holdout(spamdata$spam, ratio=.6, mode='stratified')
sd.tr = spamdata[id$tr,]
sd.te = spamdata[id$ts,]

for(j in 1:length(v.w1))
  {
  for(i in 1:length(v.w2))
    {
    # loop through each value of w to try
    w.wrf.xy = randomForest(spam~., data=sd.tr, ntree = 50, classwt=c(v.w1[j],v.w2[i])) 
  
    # get the cross validation error for each w value 
    yhat_twrf = predict(w.wrf.xy, sd.te[,-1])
    cv.for.w[j,i] = mean(yhat_twrf!=sd.te[,1])
    }
  }    

# find the optimal weight values
twrf.opt.err = min(cv.for.w)
min.err = which(cv.for.w == twrf.opt.err, arr.ind = TRUE)
w1.opt = v.w1[min.err[1]]
w2.opt = v.w2[min.err[2]]

1-twrf.opt.err
```

Compare to original
```{r}
# calculate accuracy of the 'regular' weighted random forest run earlier
mean(weighted_rf_errors$errors)
1 - mean(weighted_rf_errors$errors)
```

##C-SVM OPMTIMIZATION
Try to optimize the C-SVM model "by hand"
```{r}
#Cross Validating your C - value        
# Number of C to observe   
n.c = 20 

#  Create a sequence to try out 20 values between 2^-7 and 2^7 
v.c = seq(2^(-7),2^7, length=n.c)
cv.for.c = numeric(n.c)

for(j in 1:n.c)
{
  # loop through each value of C to try
  c.svm.xy = ksvm(spam~., data=spamdata, cross=5, C=v.c[j], 
                  type='C-svc') 
  
  # get the cross validation error for each C value 
  cv.for.c[j] = cross(c.svm.xy)
}    

# find the optimal C value
c.opt = v.c[min(which(cv.for.c==min(cv.for.c)))]

# plot the CV error for the C values
# the optimal value is in red with a line to show it's the lowest value
plot(x=v.c, y=cv.for.c, xlab='C Values', ylab='CV Error', 
     main='SVM Optimization',type='b')
points(x=c.opt,y=min(cv.for.c),col='red',pch=8)
abline(h=min(cv.for.c),col='red')
```

Let's say 27 is our optimum value of C based on the plot above
```{r}
# Use optimal value of C below to re-run SVM
csvm_spam =   ksvm(spam~., data=spamdata, cross=5, C= 27,
                     type='C-svc')

y_hat_csvm = predict(csvm_spam, spamdata[,-1])

1-mean(y_hat_csvm!=spamdata[,1])
```


```{r}
# calculate accuracy of the 'regular' C-SVM run earlier
mean(csvm_errors)
1 - mean(csvm_errors)
```

Using an optimized c-svm we get an error rate of 1.69% for an accuracy of 98.31%, which well surpasses our non-optimized c-svm accuracy of 93.91%.  This makes an optimized c-svm with C = 27 as our best performing model.

##################################################################
#
#
# FIGURES & CONCLUSION SECTION
#
#
##################################################################

##COMPARISON
Plot a comparison of the base and optimized models
```{r}
mod_names = c('RF', 'RF_Opt', 
              'WRF', 'WRF_Opt', 
              'CSVM', 'CSVM_Opt')
mod_acrcy = c(1 - mean(rf_errors$errors), 0.9502259, 
              1 - mean(weighted_rf_errors$errors), 1-twrf.opt.err,
              1 - mean(csvm_errors), 1 - mean(y_hat_csvm!=spamdata[,1]))

mod_type = c(1,1,2,2,3,3)

err_comp = data.frame(cbind(mod_names, mod_acrcy, mod_type))

ggplot(err_comp, aes(x = mod_names, y = mod_acrcy, fill = mod_type)) + geom_col()
```

